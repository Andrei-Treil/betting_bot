{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix,make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from basketball_reference_web_scraper import \n",
    "import pickle\n",
    "import random\n",
    "from client import Nba_Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list of features and samples from saved data\n",
    "def load_past_data(files):\n",
    "    '''\n",
    "    files: list of tuples in form of (features,samples,is_norm) file paths\n",
    "    returns: samples,features_norm\n",
    "    '''\n",
    "    samples = []\n",
    "    features_norm = []\n",
    "\n",
    "    for file in files:\n",
    "        features_yr = np.genfromtxt(file[0],delimiter=',')\n",
    "        if file[2]:\n",
    "            features_norm.extend(features_yr)\n",
    "        else:\n",
    "            features_yr_norm = [[float(i)/sum(j) for i in j ]for j in features_yr]\n",
    "            features_norm.extend(features_yr_norm)\n",
    "        \n",
    "        samples_yr = np.genfromtxt(file[1],delimiter=',')\n",
    "        samples.extend(samples_yr)\n",
    "\n",
    "    return samples,features_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading from saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('old_on_off_stats/2022-2023_on_off.pkl', 'rb') as f:\n",
    "    loaded_on_off = pickle.load(f)\n",
    "\n",
    "with open('old_on_off_stats/2022-2023_team_stats.pkl', 'rb') as f:\n",
    "    loaded_stats = pickle.load(f)\n",
    "\n",
    "nba_szn_2022_2023 = Nba_Season('2022','2023',team_stats=loaded_stats,team_on_off=loaded_on_off,\n",
    "                               features=np.genfromtxt('old_samps_feats/2022-2023_nba_features_inj.csv',delimiter=',')\n",
    "                               ,samples=np.genfromtxt('old_samps_feats/2022-2023_nba_samples_inj.csv',delimiter=','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating from CSV of games in form of [date, away team, away pts, home team, home pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_szn_2022_2023 = Nba_Season('2022','2023')\n",
    "nba_szn_2022_2023.pop_const_new()\n",
    "# probably need to wait before calling generate features\n",
    "features, samples = nba_szn_2022_2023.generate_features('old_games_inj/2022-2023_season_injury.csv')\n",
    "# save generated data\n",
    "nba_szn_2022_2023.save_data(save_path='old_samps_feats/')\n",
    "\n",
    "# manually save on off or team stats using commented code below\n",
    "# with open('old_on_off_stats/2022-2023_team_stats.pkl', 'wb') as f:\n",
    "#     pickle.dump(nba_szn_2022_2023.team_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2013 - 2023 NBA Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix scraper for this season!\n",
    "nba_szn_2014 = Nba_Season('2013','2014')\n",
    "nba_szn_2014.pop_const_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2014, samples_2014 = nba_szn_2014.generate_features('old_games_inj/2013-2014_season_inj.csv')\n",
    "nba_szn_2014.save_data(save_path='old_samps_feats/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load old gay boy stuff :3\n",
    "features_norm = np.genfromtxt('old_samps_feats/2015-2023_nba_features_norm_inj.csv',delimiter=',')\n",
    "samples = np.genfromtxt('old_samps_feats/2015-2023_nba_samples_inj.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2014 - 2023 NBA Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_1d = [0 if j[0] == 0 else 1 for j in samples]\n",
    "feat_train, feat_test, samp_train, samp_test = train_test_split(features_norm,samples_1d, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[3725  675  687 3550]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      4400\n",
      "           1       0.84      0.84      0.84      4237\n",
      "\n",
      "    accuracy                           0.84      8637\n",
      "   macro avg       0.84      0.84      0.84      8637\n",
      "weighted avg       0.84      0.84      0.84      8637\n",
      "\n",
      "TEST SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[738 753 684 705]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.51      1491\n",
      "           1       0.48      0.51      0.50      1389\n",
      "\n",
      "    accuracy                           0.50      2880\n",
      "   macro avg       0.50      0.50      0.50      2880\n",
      "weighted avg       0.50      0.50      0.50      2880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(16,32,64,64,32,16), alpha=0.0075, learning_rate='invscaling', activation='tanh', tol=0.001, solver='lbfgs',max_iter=1500)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TRAINING SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TEST SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN, FP, FN, TP\n",
      "[3508  597  927 3029]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82      4105\n",
      "           1       0.84      0.77      0.80      3956\n",
      "\n",
      "    accuracy                           0.81      8061\n",
      "   macro avg       0.81      0.81      0.81      8061\n",
      "weighted avg       0.81      0.81      0.81      8061\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[967 819 923 747]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.53      1786\n",
      "           1       0.48      0.45      0.46      1670\n",
      "\n",
      "    accuracy                           0.50      3456\n",
      "   macro avg       0.49      0.49      0.49      3456\n",
      "weighted avg       0.49      0.50      0.49      3456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(16,32,64,64,32,16), alpha=0.0001, learning_rate='invscaling', activation='tanh', solver='adam',max_iter=10000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TRAINING SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TEST SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[3609  496  504 3452]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      4105\n",
      "           1       0.87      0.87      0.87      3956\n",
      "\n",
      "    accuracy                           0.88      8061\n",
      "   macro avg       0.88      0.88      0.88      8061\n",
      "weighted avg       0.88      0.88      0.88      8061\n",
      "\n",
      "TEST SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[920 866 887 783]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.52      0.51      1786\n",
      "           1       0.47      0.47      0.47      1670\n",
      "\n",
      "    accuracy                           0.49      3456\n",
      "   macro avg       0.49      0.49      0.49      3456\n",
      "weighted avg       0.49      0.49      0.49      3456\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(32,64,128,64,32), alpha=0.5, learning_rate='invscaling', activation='relu', tol=0.001,solver='lbfgs',max_iter=2500)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TRAINING SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TEST SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2268 candidates, totalling 11340 fits\n",
      "Best parameters found:\n",
      " {'estimator': MLPClassifier(alpha=0.005, hidden_layer_sizes=(16, 32, 64, 32, 16),\n",
      "              learning_rate='invscaling', max_iter=10000, random_state=1), 'estimator__activation': 'relu', 'estimator__alpha': 0.005, 'estimator__early_stopping': False, 'estimator__hidden_layer_sizes': (16, 32, 64, 32, 16), 'estimator__learning_rate': 'invscaling', 'estimator__learning_rate_init': 0.001, 'estimator__max_iter': 10000, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'scaler': StandardScaler()}\n",
      "Best parameters score:\n",
      " 0.5117242965422075\n"
     ]
    }
   ],
   "source": [
    "GRID = [\n",
    "    {'scaler': [StandardScaler()],\n",
    "     'estimator': [MLPClassifier(random_state=1)],\n",
    "     'estimator__solver': ['adam'],\n",
    "     'estimator__learning_rate_init': [0.0001,0.001,0.01],\n",
    "     'estimator__learning_rate' :['invscaling','constant'],\n",
    "     'estimator__max_iter': [10000],\n",
    "     'estimator__hidden_layer_sizes': [(32,64,32), (64,64,64), (64,128,64), (32,64,64,32), (16,32,64,32,16), (32,64,128,64,32), (16,32,64,64,32,16)],\n",
    "     'estimator__activation': ['logistic', 'tanh', 'relu'],\n",
    "     'estimator__alpha': [0.0001, 0.001, 0.005],\n",
    "     'estimator__tol' : [0.01, 0.0001, 0.001],\n",
    "     'estimator__early_stopping': [True, False]\n",
    "     }\n",
    "]\n",
    "\n",
    "PIPELINE = Pipeline([('scaler', None), ('estimator', MLPClassifier())])\n",
    "\n",
    "grid_search = GridSearchCV(estimator=PIPELINE, param_grid=GRID, \n",
    "                            scoring=make_scorer(sklearn.metrics.accuracy_score),# average='macro'), \n",
    "                            n_jobs=-1, refit=True, verbose=1, \n",
    "                            return_train_score=False)\n",
    "\n",
    "grid_search.fit(feat_train,samp_train)\n",
    "print('Best parameters found:\\n', grid_search.best_params_)\n",
    "print('Best parameters score:\\n', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[3902  498  431 3806]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      4400\n",
      "           1       0.88      0.90      0.89      4237\n",
      "\n",
      "    accuracy                           0.89      8637\n",
      "   macro avg       0.89      0.89      0.89      8637\n",
      "weighted avg       0.89      0.89      0.89      8637\n",
      "\n",
      "TEST SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[743 748 672 717]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.50      0.51      1491\n",
      "           1       0.49      0.52      0.50      1389\n",
      "\n",
      "    accuracy                           0.51      2880\n",
      "   macro avg       0.51      0.51      0.51      2880\n",
      "weighted avg       0.51      0.51      0.51      2880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(128,256,256,128), alpha=0.005, learning_rate='invscaling', learning_rate_init=0.001, activation='relu', tol=0.0001,solver='adam',max_iter=10000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TRAINING SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TEST SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[3370 1030 1456 2781]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      4400\n",
      "           1       0.73      0.66      0.69      4237\n",
      "\n",
      "    accuracy                           0.71      8637\n",
      "   macro avg       0.71      0.71      0.71      8637\n",
      "weighted avg       0.71      0.71      0.71      8637\n",
      "\n",
      "TEST SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[824 667 763 626]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.54      1491\n",
      "           1       0.48      0.45      0.47      1389\n",
      "\n",
      "    accuracy                           0.50      2880\n",
      "   macro avg       0.50      0.50      0.50      2880\n",
      "weighted avg       0.50      0.50      0.50      2880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(128,256,512,256,128), alpha=0.005, learning_rate='invscaling', learning_rate_init=0.00001, activation='relu',solver='adam',max_iter=10000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TRAINING SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TEST SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022-2023 NBA Season with Injuries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_norm = []\n",
    "samples = []\n",
    "\n",
    "pop_team_stats('2023')\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "pop_team_on_off('2023')\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "# with open('2022-2023_on_off.pkl', 'wb') as f:\n",
    "#     pickle.dump(TEAM_ON_OFF, f)\n",
    "\n",
    "# with open('2022-2023_on_off.pkl', 'rb') as f:\n",
    "#     loaded_dict = pickle.load(f)\n",
    "\n",
    "features_2023,samples_2023 = generate_features('2023','old_games_inj/2022-2023_season_injury.csv')\n",
    "\n",
    "np.savetxt('2022-2023_nba_features_inj.csv', features_2023, delimiter=',')\n",
    "np.savetxt('2022-2023_nba_samples_inj.csv', samples_2023, delimiter=',')\n",
    "\n",
    "#features_2023_ext,samples_2023_ext = generate_features('2023','old_games_inj/2022-2023_season_inj_ext.csv')\n",
    "\n",
    "#features_2023.extend(features_2023_ext)\n",
    "#samples_2023.extend(samples_2023_ext)\n",
    "features_2023_norm = [[float(i)/sum(j) for i in j ]for j in features_2023]\n",
    "\n",
    "features_norm.extend(features_2023_norm)\n",
    "samples.extend(samples_2023)\n",
    "\n",
    "#features_2023_norm = [[float(i)/sum(j) for i in j ]for j in features_2023]\n",
    "samples_2023_1d = [0 if j[0] == 0 else 1 for j in samples_2023]\n",
    "feat_train, feat_test, samp_train, samp_test = train_test_split(features_2023_norm,samples_2023_1d, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN, FP, FN, TP\n",
      "[496   1   1 426]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       497\n",
      "           1       1.00      1.00      1.00       427\n",
      "\n",
      "    accuracy                           1.00       924\n",
      "   macro avg       1.00      1.00      1.00       924\n",
      "weighted avg       1.00      1.00      1.00       924\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[114 108  85  89]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54       222\n",
      "           1       0.45      0.51      0.48       174\n",
      "\n",
      "    accuracy                           0.51       396\n",
      "   macro avg       0.51      0.51      0.51       396\n",
      "weighted avg       0.52      0.51      0.51       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(16,32,64,64,32,16), alpha=0.075, learning_rate='invscaling', activation='tanh', solver='lbfgs',max_iter=10000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN, FP, FN, TP\n",
      "[451  46  14 413]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94       497\n",
      "           1       0.90      0.97      0.93       427\n",
      "\n",
      "    accuracy                           0.94       924\n",
      "   macro avg       0.93      0.94      0.93       924\n",
      "weighted avg       0.94      0.94      0.94       924\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[111 111  95  79]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.50      0.52       222\n",
      "           1       0.42      0.45      0.43       174\n",
      "\n",
      "    accuracy                           0.48       396\n",
      "   macro avg       0.48      0.48      0.48       396\n",
      "weighted avg       0.48      0.48      0.48       396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(16,32,64,64,32,16), alpha=0.0001, learning_rate='invscaling', activation='tanh', solver='adam',max_iter=10000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 256, 128)\n",
      "TN, FP, FN, TP\n",
      "[487  10   8 419]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       497\n",
      "           1       0.98      0.98      0.98       427\n",
      "\n",
      "    accuracy                           0.98       924\n",
      "   macro avg       0.98      0.98      0.98       924\n",
      "weighted avg       0.98      0.98      0.98       924\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[118 104  94  80]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.54       222\n",
      "           1       0.43      0.46      0.45       174\n",
      "\n",
      "    accuracy                           0.50       396\n",
      "   macro avg       0.50      0.50      0.50       396\n",
      "weighted avg       0.50      0.50      0.50       396\n",
      "\n",
      "(64, 128, 128, 64)\n",
      "TN, FP, FN, TP\n",
      "[488   9  29 398]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       497\n",
      "           1       0.98      0.93      0.95       427\n",
      "\n",
      "    accuracy                           0.96       924\n",
      "   macro avg       0.96      0.96      0.96       924\n",
      "weighted avg       0.96      0.96      0.96       924\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[132  90 101  73]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.59      0.58       222\n",
      "           1       0.45      0.42      0.43       174\n",
      "\n",
      "    accuracy                           0.52       396\n",
      "   macro avg       0.51      0.51      0.51       396\n",
      "weighted avg       0.51      0.52      0.52       396\n",
      "\n",
      "(128, 256, 256, 128)\n",
      "TN, FP, FN, TP\n",
      "[486  11  15 412]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       497\n",
      "           1       0.97      0.96      0.97       427\n",
      "\n",
      "    accuracy                           0.97       924\n",
      "   macro avg       0.97      0.97      0.97       924\n",
      "weighted avg       0.97      0.97      0.97       924\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[136  86 103  71]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59       222\n",
      "           1       0.45      0.41      0.43       174\n",
      "\n",
      "    accuracy                           0.52       396\n",
      "   macro avg       0.51      0.51      0.51       396\n",
      "weighted avg       0.52      0.52      0.52       396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arcs = [(128,256,128),(64,128,128,64),(128,256,256,128)]\n",
    "for arc in arcs:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=arc, alpha=0.0075, learning_rate_init=0.001, activation='tanh', solver='adam', epsilon=0.0000001, max_iter=10000)\n",
    "    mlp.fit(feat_train,samp_train)\n",
    "    print(arc)\n",
    "    predict_train = mlp.predict(feat_train)\n",
    "    predict_test = mlp.predict(feat_test)\n",
    "\n",
    "    print('TN, FP, FN, TP')\n",
    "    print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "    print(classification_report(samp_train,predict_train))\n",
    "    print('TN, FP, FN, TP')\n",
    "    print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "    print(classification_report(samp_test,predict_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       497\n",
      "           1       1.00      1.00      1.00       427\n",
      "\n",
      "    accuracy                           1.00       924\n",
      "   macro avg       1.00      1.00      1.00       924\n",
      "weighted avg       1.00      1.00      1.00       924\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.55      0.55       222\n",
      "           1       0.41      0.40      0.40       174\n",
      "\n",
      "    accuracy                           0.48       396\n",
      "   macro avg       0.48      0.48      0.48       396\n",
      "weighted avg       0.48      0.48      0.48       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(16,32,64,64,32,16), alpha=0.05, learning_rate='invscaling', activation='tanh', solver='lbfgs',max_iter=5000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 - 2023 NBA Seasons with Injuries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate Samples and Features for each Season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test Model and Tune Hyper Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_1d =  [0 if j[0] == 0 else 1 for j in samples]\n",
    "feat_train, feat_test, samp_train, samp_test = train_test_split(features_norm,samples_1d, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN, FP, FN, TP\n",
      "[2715   76   67 2448]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      2791\n",
      "           1       0.97      0.97      0.97      2515\n",
      "\n",
      "    accuracy                           0.97      5306\n",
      "   macro avg       0.97      0.97      0.97      5306\n",
      "weighted avg       0.97      0.97      0.97      5306\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[571 597 588 519]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49      1168\n",
      "           1       0.47      0.47      0.47      1107\n",
      "\n",
      "    accuracy                           0.48      2275\n",
      "   macro avg       0.48      0.48      0.48      2275\n",
      "weighted avg       0.48      0.48      0.48      2275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(16,32,64,64,32,16), alpha=0.0001, learning_rate_init=0.025, learning_rate='adaptive', activation='tanh', solver='adam', epsilon=0.0001, max_iter=10000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN, FP, FN, TP\n",
      "[2441  350 1301 1214]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.87      0.75      2791\n",
      "           1       0.78      0.48      0.60      2515\n",
      "\n",
      "    accuracy                           0.69      5306\n",
      "   macro avg       0.71      0.68      0.67      5306\n",
      "weighted avg       0.71      0.69      0.68      5306\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[824 344 788 319]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.71      0.59      1168\n",
      "           1       0.48      0.29      0.36      1107\n",
      "\n",
      "    accuracy                           0.50      2275\n",
      "   macro avg       0.50      0.50      0.48      2275\n",
      "weighted avg       0.50      0.50      0.48      2275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(16,32,64,64,32,16), alpha=0.0001, learning_rate_init=0.0025, learning_rate='adaptive', activation='relu', solver='adam', epsilon=0.000001, max_iter=5000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (64, 128, 48), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=10000)\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(16,32,16), (32,64,32), (64,64,64), (64,128,48), (16,32,64,32,16)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05, 0.001],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(mlp,parameter_space,n_jobs=-1)\n",
    "clf.fit(feat_train,samp_train)\n",
    "\n",
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 810 candidates, totalling 4050 fits\n",
      "Best parameters found:\n",
      " {'estimator': MLPClassifier(hidden_layer_sizes=(32, 64, 32), learning_rate_init=0.0001,\n",
      "              max_iter=10000, random_state=1), 'estimator__activation': 'relu', 'estimator__alpha': 0.0001, 'estimator__early_stopping': False, 'estimator__epsilon': 1e-08, 'estimator__hidden_layer_sizes': (32, 64, 32), 'estimator__learning_rate_init': 0.0001, 'estimator__max_iter': 10000, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "GRID = [\n",
    "    {'scaler': [StandardScaler()],\n",
    "     'estimator': [MLPClassifier(random_state=1)],\n",
    "     'estimator__solver': ['adam'],\n",
    "     'estimator__learning_rate_init': [0.0001],\n",
    "     'estimator__max_iter': [10000],\n",
    "     'estimator__hidden_layer_sizes': [(16,32,16), (32,64,32), (64,64,64), (64,128,48), (16,32,64,32,16)],\n",
    "     'estimator__activation': ['logistic', 'tanh', 'relu'],\n",
    "     'estimator__alpha': [0.0001, 0.001, 0.005],\n",
    "     'estimator__epsilon' : [0.001,0.00001,0.00000001],\n",
    "     'estimator__tol' : [0.01, 0.0001, 0.000001],\n",
    "     'estimator__early_stopping': [True, False]\n",
    "     }\n",
    "]\n",
    "\n",
    "PIPELINE = Pipeline([('scaler', None), ('estimator', MLPClassifier())])\n",
    "\n",
    "grid_search = GridSearchCV(estimator=PIPELINE, param_grid=GRID, \n",
    "                            scoring=make_scorer(sklearn.metrics.accuracy_score),# average='macro'), \n",
    "                            n_jobs=-1, refit=True, verbose=1, \n",
    "                            return_train_score=False)\n",
    "\n",
    "grid_search.fit(feat_train,samp_train)\n",
    "print('Best parameters found:\\n', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN, FP, FN, TP\n",
      "[2787    4   11 2504]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2791\n",
      "           1       1.00      1.00      1.00      2515\n",
      "\n",
      "    accuracy                           1.00      5306\n",
      "   macro avg       1.00      1.00      1.00      5306\n",
      "weighted avg       1.00      1.00      1.00      5306\n",
      "\n",
      "TN, FP, FN, TP\n",
      "[593 575 602 505]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.51      0.50      1168\n",
      "           1       0.47      0.46      0.46      1107\n",
      "\n",
      "    accuracy                           0.48      2275\n",
      "   macro avg       0.48      0.48      0.48      2275\n",
      "weighted avg       0.48      0.48      0.48      2275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(32,64,128,64,32), alpha=0.075, learning_rate='invscaling', activation='tanh', solver='lbfgs',max_iter=10000)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[2637  154  182 2333]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      2791\n",
      "           1       0.94      0.93      0.93      2515\n",
      "\n",
      "    accuracy                           0.94      5306\n",
      "   macro avg       0.94      0.94      0.94      5306\n",
      "weighted avg       0.94      0.94      0.94      5306\n",
      "\n",
      "TEST SET \n",
      "\n",
      "TN, FP, FN, TP\n",
      "[609 559 558 549]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52      1168\n",
      "           1       0.50      0.50      0.50      1107\n",
      "\n",
      "    accuracy                           0.51      2275\n",
      "   macro avg       0.51      0.51      0.51      2275\n",
      "weighted avg       0.51      0.51      0.51      2275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(32,64,128,64,32), alpha=0.5, learning_rate='invscaling', activation='relu', tol=0.001,solver='lbfgs',max_iter=2500)\n",
    "mlp.fit(feat_train,samp_train)\n",
    "\n",
    "predict_train = mlp.predict(feat_train)\n",
    "predict_test = mlp.predict(feat_test)\n",
    "\n",
    "print('TRAINING SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_train,predict_train).ravel())\n",
    "print(classification_report(samp_train,predict_train))\n",
    "print('TEST SET \\n')\n",
    "print('TN, FP, FN, TP')\n",
    "print(confusion_matrix(samp_test,predict_test).ravel())\n",
    "print(classification_report(samp_test,predict_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
